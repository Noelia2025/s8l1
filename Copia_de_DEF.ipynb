{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noelia2025/s8l1/blob/main/Copia_de_DEF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqLWtRwsSxCo"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "# Descargar última versión\n",
        "path = kagglehub.dataset_download(\"doanquanvietnamca/liar-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "columns = [\n",
        "    \"id\",\n",
        "    \"label\",\n",
        "    \"statement\",\n",
        "    \"subject\",\n",
        "    \"speaker\",\n",
        "    \"speaker_job\",\n",
        "    \"state\",\n",
        "    \"party\",\n",
        "    \"barely_true_counts\",\n",
        "    \"false_counts\",\n",
        "    \"half_true_counts\",\n",
        "    \"mostly_false_counts\",\n",
        "    \"mostly_true_counts\",\n",
        "    \"pants_on_fire_counts\",\n",
        "    \"context\"\n",
        "]\n",
        "\n",
        "train_path = os.path.join(path, \"train.tsv\")\n",
        "test_path = os.path.join(path, \"test.tsv\")\n",
        "valid_path = os.path.join(path, \"valid.tsv\")\n",
        "\n",
        "train = pd.read_csv(train_path, sep=\"\\t\", header=None, names=columns, quoting=3)\n",
        "test = pd.read_csv(test_path, sep=\"\\t\", header=None, names=columns, quoting=3)\n",
        "valid = pd.read_csv(valid_path, sep=\"\\t\", header=None, names=columns, quoting=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD0A_RsbTGvC"
      },
      "outputs": [],
      "source": [
        "#valores numéricos para regresión\n",
        "label_map = {\n",
        "    \"pants-fire\": 0,\n",
        "    \"false\": 1,\n",
        "    \"barely-true\": 2,\n",
        "    \"half-true\": 3,\n",
        "    \"mostly-true\": 4,\n",
        "    \"true\": 5\n",
        "}\n",
        "\n",
        "train[\"target\"] = train[\"label\"].map(label_map)\n",
        "valid[\"target\"] = valid[\"label\"].map(label_map)\n",
        "test[\"target\"] = test[\"label\"].map(label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr7sASxPUqzI"
      },
      "outputs": [],
      "source": [
        "X_train = train[\"statement\"]\n",
        "y_train = train[\"target\"]\n",
        "\n",
        "X_valid = valid[\"statement\"]\n",
        "y_valid = valid[\"target\"]\n",
        "\n",
        "X_test = test[\"statement\"]\n",
        "y_test = test[\"target\"]\n",
        "\n",
        "print(train[\"label\"].value_counts())\n",
        "print(train[\"target\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análisis exploratorio"
      ],
      "metadata": {
        "id": "I4KhGvZzW4l5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Función para tokenizar\n",
        "#Limpia minúsculas , stopwords, palabras muy cortas y caracteres especiales\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
        "    return tokens\n",
        "\n",
        "# Estadísticas básicas\n",
        "print(\" INFORMACIÓN BÁSICA DEL DATASET \")\n",
        "print(f\"Número de instancias y variables: {train.shape}\")\n",
        "print(\"\\nTipos de datos de cada columna:\")\n",
        "print(train.dtypes)\n",
        "print(\"\\nEstadísticas descriptivas de conteos de veracidad:\")\n",
        "count_cols = [\"barely_true_counts\",\"false_counts\",\"half_true_counts\",\n",
        "              \"mostly_false_counts\",\"mostly_true_counts\",\"pants_on_fire_counts\"]\n",
        "print(train[count_cols].describe())\n",
        "\n",
        "print(\"\\nVALORES NULOS POR COLUMNA\")\n",
        "print(train.isnull().sum())\n",
        "\n",
        "#Distribución del target\n",
        "print(\"\\n HISTOGRAMA DEL TARGET \")\n",
        "print(\"Visualización de la distribución de clases en la columna 'target'.\")\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(train[\"target\"], bins=len(train[\"target\"].unique()), kde=False)\n",
        "plt.title(\"Distribución del target\")\n",
        "plt.xlabel(\"Target\")\n",
        "plt.ylabel(\"Número de instancias\")\n",
        "plt.show()\n",
        "\n",
        "# Longitud de los textos\n",
        "train[\"len\"] = train[\"statement\"].str.split().apply(len)\n",
        "\n",
        "print(\"\\nHISTOGRAMA DE LONGITUD DE LOS TEXTOS\")\n",
        "print(\"Muestra cuántas palabras tienen los statements.\")\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(train[\"len\"], bins=50, kde=False)\n",
        "plt.title(\"Distribución de longitudes de texto\")\n",
        "plt.xlabel(\"Número de palabras\")\n",
        "plt.ylabel(\"Número de instancias\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- BOXPLOT DE LONGITUD DE TEXTOS POR TARGET ---\")\n",
        "print(\"Permite comparar la longitud de los statements según la clase del target.\")\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(x=\"target\", y=\"len\", data=train)\n",
        "plt.title(\"Longitud de los textos por target\")\n",
        "plt.xlabel(\"Target\")\n",
        "plt.ylabel(\"Número de palabras\")\n",
        "plt.show()\n",
        "\n",
        "# Nube de palabras por target\n",
        "print(\"\\n--- NUBES DE PALABRAS POR TARGET ---\")\n",
        "print(\"Cada nube muestra las palabras más frecuentes para cada clase del target.\")\n",
        "for t in sorted(train[\"target\"].unique()):\n",
        "    texts = \" \".join(train[train[\"target\"] == t][\"statement\"])\n",
        "    wc = WordCloud(width=1200, height=800, background_color=\"white\").generate(texts)\n",
        "\n",
        "    print(f\"\\nMostrando nube de palabras para Target: {t}\")\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Nube de palabras – Target {t}\")\n",
        "    plt.show()\n",
        "\n",
        "# Palabras más frecuentes por target\n",
        "print(\"\\n--- TOP 20 PALABRAS MÁS FRECUENTES POR TARGET ---\")\n",
        "print(\"Estos gráficos muestran las 20 palabras más repetidas por cada clase del target.\")\n",
        "for t in sorted(train[\"target\"].unique()):\n",
        "    subset = train[train[\"target\"] == t]\n",
        "    all_text = \" \".join(subset[\"statement\"].astype(str))\n",
        "    tokens = tokenize(all_text)\n",
        "    counter = Counter(tokens)\n",
        "    top_words = counter.most_common(20)\n",
        "    df_top = pd.DataFrame(top_words, columns=[\"word\", \"count\"]).sort_values(\"count\", ascending=True)\n",
        "\n",
        "    print(f\"\\nMostrando top 20 palabras para Target: {t}\")\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.barplot(data=df_top, x=\"count\", y=\"word\", hue=\"word\", palette=\"viridis\", legend=False)\n",
        "    plt.title(f\"Top 20 palabras más frecuentes – Target {t}\")\n",
        "    plt.xlabel(\"Frecuencia\")\n",
        "    plt.ylabel(\"Palabra\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RfyODzkUWvzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   True - valor 5 - la declaración es precisa\n",
        "*   Mostly-true - valor 4 - la declaración contiene un elemento de verdad, pero puede faltarle contexto\n",
        "*   Half-True - valor 3 - la declaración es parcialmente verdadera y parcilmente falsa\n",
        "*   Barely-True - valor 2 - la declaración NO es precisa\n",
        "*   Pants-Fire - valor 0 - la declaración es MUY falsa\n",
        "\n"
      ],
      "metadata": {
        "id": "qJ8zWbSqUJyM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dPk0_rFVjIg"
      },
      "source": [
        "Ridge + TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppa3wvW9VhCP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=15000,\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1,3),\n",
        "    min_df=3,\n",
        "    max_df=0.85,\n",
        "    sublinear_tf=True,\n",
        "    norm='l2'\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf  = tfidf.transform(X_test)\n",
        "\n",
        "# Ridge con validación\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train_tfidf, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Buscar mejor alpha con validación simple\n",
        "best_alpha = None\n",
        "best_score = -np.inf\n",
        "\n",
        "alphas = np.logspace(-2, 2, 10)\n",
        "for alpha in alphas:\n",
        "    ridge = Ridge(alpha=alpha, solver='sparse_cg', max_iter=1000)\n",
        "    ridge.fit(X_tr, y_tr)\n",
        "    score = ridge.score(X_val, y_val)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_alpha = alpha\n",
        "\n",
        "print(f\"Mejor alpha encontrado: {best_alpha}\")\n",
        "\n",
        "# Modelo final\n",
        "ridge_tfidf = Ridge(alpha=best_alpha, solver='sparse_cg', max_iter=1000)\n",
        "ridge_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "pred_tfidf = ridge_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "# Resultados\n",
        "print(\"\\nTF-IDF OPTIMIZED RESULTS\")\n",
        "print(\"Mejor alpha:\", best_alpha)\n",
        "print(\"MSE:\", mean_squared_error(y_test, pred_tfidf))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, pred_tfidf))\n",
        "print(\"R2:\", r2_score(y_test, pred_tfidf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jofWyny3sqAq"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tai_tiPiWWUA"
      },
      "source": [
        "Ridge + Word2Vec (promedio de embeddings) quedarse con el segundo q lo mejora bastante"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV4j24G8WjEA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 1. Tokenización\n",
        "# Mantiene números y preserva palabras importantes\n",
        "\n",
        "def improved_tokenize(text):\n",
        "    text = text.lower()\n",
        "    # Preservar números y letras y separar puntuación\n",
        "    text = re.sub(r\"([.,!?;:])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-z0-9\\s.,!?;:]\", \"\", text)\n",
        "    tokens = text.split()\n",
        "    # Filtrar stopwords pero mantener tokens significativos\n",
        "    tokens = [t for t in tokens if t and (len(t) > 2 or t.isdigit())]\n",
        "    return tokens\n",
        "\n",
        "print(\"Tokenizando textos...\")\n",
        "train_tokens = [improved_tokenize(str(text)) for text in X_train]\n",
        "valid_tokens = [improved_tokenize(str(text)) for text in X_valid]\n",
        "test_tokens = [improved_tokenize(str(text)) for text in X_test]\n",
        "\n",
        "\n",
        "# 2. Promedio ponderado por TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Calcular TF-IDF para usar como pesos\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=None,\n",
        "    min_df=3,\n",
        "    tokenizer=lambda x: x,\n",
        "    preprocessor=lambda x: x,\n",
        "    token_pattern=None\n",
        ")\n",
        "\n",
        "# Convertir tokens a strings para TfidfVectorizer\n",
        "train_tokens_str = [' '.join(tokens) for tokens in train_tokens]\n",
        "valid_tokens_str = [' '.join(tokens) for tokens in valid_tokens]\n",
        "test_tokens_str = [' '.join(tokens) for tokens in test_tokens]\n",
        "\n",
        "tfidf_vectorizer.fit(train_tokens_str)\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "def weighted_avg_w2v(tokens, model, tfidf_vec, vector_size=300):\n",
        "\n",
        "    # Obtener pesos TF-IDF\n",
        "    text_str = ' '.join(tokens)\n",
        "    try:\n",
        "        tfidf_scores = tfidf_vec.transform([text_str]).toarray()[0]\n",
        "        feature_names = tfidf_vec.get_feature_names_out()\n",
        "        word_to_tfidf = dict(zip(feature_names, tfidf_scores))\n",
        "    except:\n",
        "        word_to_tfidf = {}\n",
        "\n",
        "    vectors = []\n",
        "    weights = []\n",
        "\n",
        "    for word in tokens:\n",
        "        if word in model.wv:\n",
        "            vectors.append(model.wv[word])\n",
        "            weights.append(word_to_tfidf.get(word, 1.0))\n",
        "\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "    vectors = np.array(vectors)\n",
        "    weights = np.array(weights).reshape(-1, 1)\n",
        "\n",
        "    # Promedio ponderado\n",
        "    weighted_vectors = vectors * weights\n",
        "    return np.sum(weighted_vectors, axis=0) / np.sum(weights)\n",
        "\n",
        "# Ensemble de W2V\n",
        "\n",
        "print(\"PROBANDO ENSEMBLE DE WORD2VEC\")\n",
        "\n",
        "\n",
        "configs = [\n",
        "    {'vector_size': 200, 'window': 5, 'sg': 0, 'name': 'CBOW-200'},\n",
        "    {'vector_size': 300, 'window': 10, 'sg': 1, 'name': 'SkipGram-300'},\n",
        "    {'vector_size': 400, 'window': 7, 'sg': 1, 'name': 'SkipGram-400'},\n",
        "]\n",
        "\n",
        "predictions_train = []\n",
        "predictions_test = []\n",
        "\n",
        "for i, config in enumerate(configs, 1):\n",
        "    name = config.pop('name')\n",
        "    print(f\"\\n[{i}/3] Entrenando configuración: {name}\")\n",
        "\n",
        "    model = Word2Vec(\n",
        "        sentences=train_tokens,\n",
        "        min_count=3,\n",
        "        workers=4,\n",
        "        epochs=20,\n",
        "        **config\n",
        "    )\n",
        "\n",
        "    vector_size = config['vector_size']\n",
        "\n",
        "    # Generar embeddings\n",
        "    X_tr = np.array([weighted_avg_w2v(t, model, tfidf_vectorizer, vector_size)\n",
        "                     for t in train_tokens])\n",
        "    X_te = np.array([weighted_avg_w2v(t, model, tfidf_vectorizer, vector_size)\n",
        "                     for t in test_tokens])\n",
        "\n",
        "    # Escalar\n",
        "    scaler = StandardScaler()\n",
        "    X_tr_sc = scaler.fit_transform(X_tr)\n",
        "    X_te_sc = scaler.transform(X_te)\n",
        "\n",
        "    # Entrenar Ridge\n",
        "    ridge = Ridge(alpha=best_alpha)  # alpha ya optimizado\n",
        "    ridge.fit(X_tr_sc, y_train)\n",
        "\n",
        "    predictions_train.append(ridge.predict(X_tr_sc))\n",
        "    predictions_test.append(ridge.predict(X_te_sc))\n",
        "\n",
        "    # Mostrar resultado individual\n",
        "    r2_individual = r2_score(y_test, predictions_test[-1])\n",
        "    print(f\"  R2 individual: {r2_individual:.4f}\")\n",
        "\n",
        "# Promedio de predicciones (ensemble simple)\n",
        "final_pred_test = np.mean(predictions_test, axis=0)\n",
        "\n",
        "mse_ensemble = mean_squared_error(y_test, final_pred_test)\n",
        "mae_ensemble = mean_absolute_error(y_test, final_pred_test)\n",
        "r2_ensemble = r2_score(y_test, final_pred_test)\n",
        "\n",
        "print(\"\\n===== ENSEMBLE W2V RESULTS =====\")\n",
        "print(f\"MSE: {mse_ensemble:.4f}\")\n",
        "print(f\"MAE: {mae_ensemble:.4f}\")\n",
        "print(f\"R2: {r2_ensemble:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjbHl_QkWlM9"
      },
      "source": [
        "3. Ridge + BERT embeddings (sin fine-tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWfA-7VnWnkh"
      },
      "outputs": [],
      "source": [
        "print(\"Generando embeddings ...\")\n",
        "X_train_bert = bert_model.encode(list(X_train), show_progress_bar=True)\n",
        "X_test_bert  = bert_model.encode(list(X_test), show_progress_bar=True)\n",
        "\n",
        "# 2. Escalar (normalizar)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_bert)\n",
        "X_test_scaled  = scaler.transform(X_test_bert)\n",
        "\n",
        "# 3. Verificar tamaños (deben ser 10269 train y 1283 test)\n",
        "print(f\"Tamaño X_train_scaled: {X_train_scaled.shape}\")\n",
        "print(f\"Tamaño X_test_scaled: {X_test_scaled.shape}\")\n",
        "print(f\"Tamaño y_test: {y_test.shape}\")\n",
        "\n",
        "# 4. Entrenar ridge y evaluar\n",
        "alphas = np.logspace(-3, 3, 20)\n",
        "ridge_bert = RidgeCV(alphas=alphas, scoring=\"r2\")\n",
        "ridge_bert.fit(X_train_scaled, y_train)\n",
        "\n",
        "pred_bert = ridge_bert.predict(X_test_scaled)\n",
        "\n",
        "# resultados\n",
        "print(\"\\n===== BERT EMBEDDINGS RESULTS (CORREGIDO) =====\")\n",
        "print(\"MSE:\", mean_squared_error(y_test, pred_bert))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, pred_bert))\n",
        "print(\"R2:\", r2_score(y_test, pred_bert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gxS9UGeqJeC"
      },
      "source": [
        "# RED NEURONAL EN PYTORCH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Red Neuronal + TF-IDF"
      ],
      "metadata": {
        "id": "d7ZVIwNFiYX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "#COMPRESIÓN EXTREMA (sino MALOS resultados)\n",
        "print(\"1. Generando TF-IDF y Comprimiendo a SOLO 50 componentes...\")\n",
        "\n",
        "# TF-IDF base\n",
        "tfidf = TfidfVectorizer(max_features=2000, stop_words=\"english\", ngram_range=(1,2))\n",
        "X_train_sparse = tfidf.fit_transform(X_train)\n",
        "X_test_sparse  = tfidf.transform(X_test)\n",
        "\n",
        "# SVD a 50 componentes (cuello de botella extremo)\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "X_train_lsa = svd.fit_transform(X_train_sparse)\n",
        "X_test_lsa  = svd.transform(X_test_sparse)\n",
        "\n",
        "# Escalado\n",
        "scaler = StandardScaler()\n",
        "X_train_lsa = scaler.fit_transform(X_train_lsa)\n",
        "X_test_lsa  = scaler.transform(X_test_lsa)\n",
        "\n",
        "print(f\"Dimensiones de entrada: {X_train_lsa.shape}\")\n",
        "\n",
        "#TENSORES\n",
        "X_train_tensor = torch.tensor(X_train_lsa.astype(np.float32), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values.astype(np.float32), dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor  = torch.tensor(X_test_lsa.astype(np.float32), dtype=torch.float32)\n",
        "y_test_tensor  = torch.tensor(y_test.values.astype(np.float32), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Batch size grande para gradientes más estables\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=128, shuffle=False)\n",
        "\n",
        "# RED NEURONAL\n",
        "class MicroNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MicroNN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # Entrada 50 -> 16 (Muy pequeña)\n",
        "            nn.Linear(input_size, 16),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.6), # ¡Apagamos el 60% de las neuronas!\n",
        "\n",
        "            # Salida directa\n",
        "            nn.Linear(16, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MicroNN(input_size=50).to(device)\n",
        "print(f\"Modelo MicroNN cargado en: {device}\")\n",
        "\n",
        "# ENTRENAMIENTO FUERTEMENTE REGULARIZADO\n",
        "criterion = nn.MSELoss()\n",
        "# weight_decay muy alto (0.05) para forzar pesos pequeños\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay=0.05)\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "print(\"\\nIniciando entrenamiento\")\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validación\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "    model.train()\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {running_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(test_loader):.4f}\")\n",
        "\n",
        "# EVALUACIÓN\n",
        "model.eval()\n",
        "predictions, actuals = [], []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy().flatten())\n",
        "        actuals.extend(labels.numpy().flatten())\n",
        "\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "r2 = r2_score(actuals, predictions)\n",
        "\n",
        "print(\"\\nRESULTADO FINAL (TF-IDF + MicroNN)\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"R2: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "F1n-BW1bhPTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Red Neuronal + Word2Vec"
      ],
      "metadata": {
        "id": "WGu4LHKpkjhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "#GENERAR FEATURES (Configuración SkipGram-300)\n",
        "print(\"Generando embeddings Word2Vec para la Red Neuronal...\")\n",
        "\n",
        "w2v_model_nn = Word2Vec(\n",
        "    sentences=train_tokens,\n",
        "    vector_size=300,\n",
        "    window=10,\n",
        "    min_count=3,\n",
        "    workers=4,\n",
        "    epochs=20,\n",
        "    sg=1\n",
        ")\n",
        "\n",
        "# Generar vectores promedio ponderados por TF-IDF\n",
        "X_train_w2v = np.array([weighted_avg_w2v(t, w2v_model_nn, tfidf_vectorizer, 300) for t in train_tokens])\n",
        "X_test_w2v  = np.array([weighted_avg_w2v(t, w2v_model_nn, tfidf_vectorizer, 300) for t in test_tokens])\n",
        "\n",
        "# Escalar\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_w2v)\n",
        "X_test_scaled  = scaler.transform(X_test_w2v)\n",
        "\n",
        "print(f\"Dimensiones de entrada: {X_train_scaled.shape}\")\n",
        "\n",
        "# PREPARAR PYTORCH\n",
        "# Convertir a Tensores\n",
        "X_train_tensor = torch.tensor(X_train_scaled.astype(np.float32), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values.astype(np.float32), dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor  = torch.tensor(X_test_scaled.astype(np.float32), dtype=torch.float32)\n",
        "y_test_tensor  = torch.tensor(y_test.values.astype(np.float32), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ARQUITECTURA DE LA RED\n",
        "class W2V_NN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(W2V_NN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # Entrada 300 -> 128\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.BatchNorm1d(128), # Estabilidad\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),# Regularización\n",
        "\n",
        "            # 128 -> 64\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            # Salida (Regresión)\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = W2V_NN(input_size=300).to(device)\n",
        "print(f\"Modelo W2V-NN cargado en: {device}\")\n",
        "\n",
        "# ENTRENAMIENTO\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "EPOCHS = 30\n",
        "\n",
        "print(\"\\nIniciando entrenamiento Word2Vec NN\")\n",
        "model.train()\n",
        "best_loss = float('inf')\n",
        "patience = 5\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validación simple\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "    model.train()\n",
        "\n",
        "    avg_train = running_loss/len(train_loader)\n",
        "    avg_val = val_loss/len(test_loader)\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f}\")\n",
        "\n",
        "    # Early Stopping básico\n",
        "    if avg_val < best_loss:\n",
        "        best_loss = avg_val\n",
        "        torch.save(model.state_dict(), 'best_w2v_model.pth')\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early Stopping activado.\")\n",
        "            break\n",
        "\n",
        "# EVALUACIÓN\n",
        "model.load_state_dict(torch.load('best_w2v_model.pth', weights_only=True))\n",
        "model.eval()\n",
        "predictions, actuals = [], []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy().flatten())\n",
        "        actuals.extend(labels.numpy().flatten())\n",
        "\n",
        "mse_nn = mean_squared_error(actuals, predictions)\n",
        "mae_nn = mean_absolute_error(actuals, predictions)\n",
        "r2_nn = r2_score(actuals, predictions)\n",
        "\n",
        "print(\"\\nPYTORCH NN (Word2Vec) RESULTS \")\n",
        "print(f\"MSE: {mse_nn:.4f}\")\n",
        "print(f\"MAE: {mae_nn:.4f}\")\n",
        "print(f\"R2: {r2_nn:.4f}\")"
      ],
      "metadata": {
        "id": "FsVfjzIphSdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Red Neuronal + BERT embeddings"
      ],
      "metadata": {
        "id": "TvwUZktTmx73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# PREPARAR DATOS\n",
        "print(f\"Usando embeddings de BERT: {X_train_scaled.shape}\")\n",
        "\n",
        "# Convertir a tensores de PyTorch\n",
        "X_train_tensor = torch.tensor(X_train_scaled.astype(np.float32), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values.astype(np.float32), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled.astype(np.float32), dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values.astype(np.float32), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Crear dataLoaders (Batch size 32)\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ARQUITECTURA (BERT-MLP)\n",
        "class BERT_NN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(BERT_NN, self).__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            # Capa 1: Entrada (384) -> 256\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.BatchNorm1d(256), # Normalización por lotes para estabilidad\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),  # Dropout para evitar overfitting\n",
        "\n",
        "            # Capa 2: 256 -> 64\n",
        "            nn.Linear(256, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            # Salida: 64 -> 1\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Inicializar modelo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_dim = X_train_scaled.shape[1] # Debería ser 384 (si usaste MiniLM) o 768\n",
        "model = BERT_NN(input_dim).to(device)\n",
        "print(f\"Modelo BERT-NN cargado en: {device}\")\n",
        "\n",
        "# ENTRENAMIENTO\n",
        "criterion = nn.MSELoss()\n",
        "# AdamW suele funcionar mejor que Adam estándar para embeddings\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "EPOCHS = 25\n",
        "best_loss = float('inf')\n",
        "patience = 5\n",
        "counter = 0\n",
        "\n",
        "print(\"\\nIniciando entrenamiento BERT NN\")\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validación (Test set)\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "    model.train()\n",
        "\n",
        "    avg_train = running_loss / len(train_loader)\n",
        "    avg_val = val_loss / len(test_loader)\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train:.4f} | Val Loss: {avg_val:.4f}\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if avg_val < best_loss:\n",
        "        best_loss = avg_val\n",
        "        torch.save(model.state_dict(), 'best_bert_nn.pth')\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Deteniendo por Early Stopping.\")\n",
        "            break\n",
        "\n",
        "# EVALUACIÓN FINAL\n",
        "model.load_state_dict(torch.load('best_bert_nn.pth', weights_only=True))\n",
        "model.eval()\n",
        "predictions, actuals = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy().flatten())\n",
        "        actuals.extend(labels.numpy().flatten())\n",
        "\n",
        "mse_nn = mean_squared_error(actuals, predictions)\n",
        "mae_nn = mean_absolute_error(actuals, predictions)\n",
        "r2_nn = r2_score(actuals, predictions)\n",
        "\n",
        "print(\"\\nPYTORCH NN (BERT Embeddings) RESULTS\")\n",
        "print(f\"MSE: {mse_nn:.4f}\")\n",
        "print(f\"MAE: {mae_nn:.4f}\")\n",
        "print(f\"R2: {r2_nn:.4f}\")"
      ],
      "metadata": {
        "id": "0WqJ8TpvhVPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viZex0jRqZiv"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets evaluate accelerate\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# PREPARACIÓN DE DATOS\n",
        "# Creamos un DataFrame temporal\n",
        "df_train = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
        "df_valid = pd.DataFrame({\"text\": X_valid, \"label\": y_valid})\n",
        "df_test  = pd.DataFrame({\"text\": X_test,  \"label\": y_test})\n",
        "\n",
        "# Convertimos a formato Dataset de HF\n",
        "dataset_train = Dataset.from_pandas(df_train)\n",
        "dataset_valid = Dataset.from_pandas(df_valid)\n",
        "dataset_test  = Dataset.from_pandas(df_test)\n",
        "\n",
        "# Tokenización\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train = dataset_train.map(tokenize_function, batched=True)\n",
        "tokenized_valid = dataset_valid.map(tokenize_function, batched=True)\n",
        "tokenized_test  = dataset_test.map(tokenize_function, batched=True)\n",
        "\n",
        "# Preparamos formato para PyTorch (labels deben ser float para regresión)\n",
        "tokenized_train = tokenized_train.map(lambda x: {\"labels\": float(x[\"label\"])})\n",
        "tokenized_valid = tokenized_valid.map(lambda x: {\"labels\": float(x[\"label\"])})\n",
        "tokenized_test  = tokenized_test.map(lambda x: {\"labels\": float(x[\"label\"])})\n",
        "\n",
        "# Eliminamos columnas innecesarias\n",
        "cols_to_remove = [\"text\", \"label\", \"__index_level_0__\"]\n",
        "tokenized_train = tokenized_train.remove_columns([c for c in cols_to_remove if c in tokenized_train.column_names])\n",
        "tokenized_valid = tokenized_valid.remove_columns([c for c in cols_to_remove if c in tokenized_valid.column_names])\n",
        "tokenized_test  = tokenized_test.remove_columns([c for c in cols_to_remove if c in tokenized_test.column_names])\n",
        "\n",
        "tokenized_train.set_format(\"torch\")\n",
        "tokenized_valid.set_format(\"torch\")\n",
        "tokenized_test.set_format(\"torch\")\n",
        "\n",
        "#MODELO\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n",
        "\n",
        "#MÉTRICAS\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    mse = mean_squared_error(labels, predictions)\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    r2 = r2_score(labels, predictions)\n",
        "\n",
        "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2}\n",
        "\n",
        "#ENTRENAMIENTO\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_finetuned_regression\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"mse\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Iniciando Fine-Tuning de BERT (Regresión)...\")\n",
        "trainer.train()\n",
        "\n",
        "# EVALUACIÓN FINAL\n",
        "print(\"\\nEvaluando en TEST...\")\n",
        "results = trainer.predict(tokenized_test)\n",
        "\n",
        "print(\"\\nFINE-TUNED BERT RESULTS (Comparable con anteriores)\")\n",
        "print(f\"MSE: {results.metrics['test_mse']:.4f}\")\n",
        "print(f\"MAE: {results.metrics['test_mae']:.4f}\")\n",
        "print(f\"R2:  {results.metrics['test_r2']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMzgjcYMLnpa"
      },
      "source": [
        "PROYECTO DE EXTENSIÓN (ANÁLISIS CORRELACIONAL DE POLARIZACIÓN +0.75 PTOS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCmhL6NQEe0J"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# LIMPIEZA PREVIA ESPECÍFICA PARA LA EXTENSIÓN\n",
        "# Aseguramos que las columnas de conteo sean números.\n",
        "# 'coerce' convierte cualquier texto raro en NaN, y fillna(0) lo pone a cero.\n",
        "cols_to_fix = ['false_counts', 'barely_true_counts', 'pants_on_fire_counts']\n",
        "\n",
        "for col in cols_to_fix:\n",
        "    train[col] = pd.to_numeric(train[col], errors='coerce').fillna(0)\n",
        "\n",
        "# AGRUPACIÓN Y CÁLCULO\n",
        "train_party_stats = train.groupby('party').agg(\n",
        "    total_statements=('id', 'count'),\n",
        "    false_counts=('false_counts', 'sum'),\n",
        "    barely_true_counts=('barely_true_counts', 'sum'),\n",
        "    pants_on_fire_counts=('pants_on_fire_counts', 'sum'),\n",
        ").reset_index()\n",
        "\n",
        "# CÁLCULO DEL RATIO\n",
        "train_party_stats['fake_ratio'] = (\n",
        "    train_party_stats['false_counts'] +\n",
        "    train_party_stats['barely_true_counts'] +\n",
        "    train_party_stats['pants_on_fire_counts']\n",
        ") / train_party_stats['total_statements']\n",
        "\n",
        "# VISUALIZACIÓN\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Filtramos partidos con más de 50 declaraciones para evitar ruido\n",
        "df_plot = train_party_stats[train_party_stats['total_statements'] > 50].sort_values('fake_ratio', ascending=False)\n",
        "\n",
        "sns.barplot(\n",
        "    x='party',\n",
        "    y='fake_ratio',\n",
        "    data=df_plot,\n",
        "    palette='viridis' # Añadí una paleta de colores para que se vea mejor\n",
        ")\n",
        "plt.title(\"Ratio de Afirmaciones Falsas/Casi Falsas por Partido (Polarización)\")\n",
        "plt.ylabel(\"Ratio de Afirmaciones de Baja Credibilidad\")\n",
        "plt.xlabel(\"Partido\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}